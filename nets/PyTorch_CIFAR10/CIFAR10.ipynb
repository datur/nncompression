{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning PyTorch vision models to work with CIFAR-10 dataset\n",
    "### Author: Huy Phan\n",
    "### Github: https://github.com/huyvnphan/PyTorch-CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm import tqdm as pbar\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from cifar10_models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataloaders(params):\n",
    "    \"\"\"\n",
    "    Make a Pytorch dataloader object that can be used for traing and valiation\n",
    "    Input:\n",
    "        - params dict with key 'path' (string): path of the dataset folder\n",
    "        - params dict with key 'batch_size' (int): mini-batch size\n",
    "        - params dict with key 'num_workers' (int): number of workers for dataloader\n",
    "    Output:\n",
    "        - trainloader and testloader (pytorch dataloader object)\n",
    "    \"\"\"\n",
    "    transform_train = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
    "                                          transforms.RandomHorizontalFlip(),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n",
    "    \n",
    "    transform_validation = transforms.Compose([transforms.ToTensor(),\n",
    "                                               transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n",
    "    \n",
    "    transform_validation = transforms.Compose([transforms.ToTensor()])\n",
    "    \n",
    "    trainset = torchvision.datasets.CIFAR10(root=params['path'], train=True, transform=transform_train)\n",
    "    testset = torchvision.datasets.CIFAR10(root=params['path'], train=False, transform=transform_validation)\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=params['batch_size'], shuffle=True, num_workers=4)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=params['batch_size'], shuffle=False, num_workers=4)\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, params):\n",
    "    \n",
    "    writer = SummaryWriter('runs/' + params['description'])\n",
    "    model = model.to(params['device'])\n",
    "    optimizer = optim.AdamW(model.parameters())\n",
    "    total_updates = params['num_epochs']*len(params['train_loader'])\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_accuracy = test_model(model, params)\n",
    "    best_model = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "    for epoch in pbar(range(params['num_epochs'])):\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'validation']:\n",
    "            \n",
    "            # Loss accumulator for each epoch\n",
    "            logs = {'Loss': 0.0,\n",
    "                    'Accuracy': 0.0}\n",
    "            \n",
    "            # Set the model to the correct phase\n",
    "            model.train() if phase == 'train' else model.eval()\n",
    "            \n",
    "            # Iterate over data\n",
    "            for image, label in params[phase+'_loader']:\n",
    "                image = image.to(params['device'])\n",
    "                label = label.to(params['device'])\n",
    "                \n",
    "                # Zero gradient\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    prediction = model(image)\n",
    "                    loss = criterion(prediction, label)\n",
    "                    accuracy = torch.sum(torch.max(prediction, 1)[1] == label.data).item()\n",
    "\n",
    "                    # Update log\n",
    "                    logs['Loss'] += image.shape[0]*loss.detach().item()\n",
    "                    logs['Accuracy'] += accuracy\n",
    "\n",
    "                    # Backward pass\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "            \n",
    "            # Normalize and write the data to TensorBoard\n",
    "            logs['Loss'] /= len(params[phase+'_loader'].dataset)\n",
    "            logs['Accuracy'] /= len(params[phase+'_loader'].dataset)\n",
    "            writer.add_scalars('Loss', {phase: logs['Loss']}, epoch)\n",
    "            writer.add_scalars('Accuracy', {phase: logs['Accuracy']}, epoch)\n",
    "\n",
    "            # Save the best weights\n",
    "            if phase == 'validation' and logs['Accuracy'] > best_accuracy:\n",
    "                best_accuracy = logs['Accuracy']\n",
    "                best_model = copy.deepcopy(model.state_dict())\n",
    "                 \n",
    "        # Write best weights to disk\n",
    "        if epoch % params['check_point'] == 0 or epoch == params['num_epochs']-1:\n",
    "            torch.save(best_model, params['description'] + '.pt')\n",
    "    \n",
    "    final_accuracy = test_model(model, params)\n",
    "    writer.add_text('Final_Accuracy', str(final_accuracy), 0)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, params):\n",
    "    \n",
    "    model = model.to(params['device']).eval()     \n",
    "    phase = 'validation'\n",
    "    logs = {'Accuracy': 0.0}\n",
    "            \n",
    "    # Iterate over data\n",
    "    for image, label in pbar(params[phase+'_loader']):\n",
    "        image = image.to(params['device'])\n",
    "        label = label.to(params['device'])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = model(image)\n",
    "            accuracy = torch.sum(torch.max(prediction, 1)[1] == label.data).item()\n",
    "            logs['Accuracy'] += accuracy\n",
    "            \n",
    "    logs['Accuracy'] /= len(params[phase+'_loader'].dataset)\n",
    "    \n",
    "    return logs['Accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create PyTorch models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Put everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on cuda if available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params = {'path': '/raid/data/pytorch_dataset/cifar10', 'batch_size': 256}\n",
    "\n",
    "train_loader, validation_loader = make_dataloaders(data_params)\n",
    "\n",
    "train_params = {'description': 'Test',\n",
    "                'num_epochs': 300,\n",
    "                'check_point': 50, 'device': device,\n",
    "                'train_loader': train_loader, 'validation_loader': validation_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:zalo]",
   "language": "python",
   "name": "conda-env-zalo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
