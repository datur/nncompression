{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import neptune\n",
    "import torch\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from time import perf_counter_ns\n",
    "import nncompression.utils as nnu\n",
    "from nncompression.experiments import utils as exu\n",
    "from nncompression.models.pytorch.utils import get_imagenet_val_loader\n",
    "from nncompression.utils import IMAGENET_LABELS, DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"http://192.168.0.28:5000/\"\n",
    "\n",
    "r = requests.get(BASE_URL+'avail_models')\n",
    "AVAILABLE_MODELS = r.json()['available_models']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AVAILABLE_MODELS[3]\n",
    "\n",
    "r = requests.post(BASE_URL+f'set_model/{model}', json={'api_key': 'password'})\n",
    "model_info = r.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'resnet101',\n",
       " 'non-trainable_params': 0,\n",
       " 'total_input_size': 0.57421875,\n",
       " 'total_output_size': 429.73028564453125,\n",
       " 'total_params': 44549160,\n",
       " 'total_params_size': 169.94155883789062,\n",
       " 'total_size': 600.2460632324219,\n",
       " 'trainable_params': 44549160}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NVMLError: NVML Shared Library Not Found - GPU usage metrics may not be reported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ui.neptune.ai/davidturner94/deployment-eval/e/DEP-4\n"
     ]
    }
   ],
   "source": [
    "neptune.init('davidturner94/deployment-eval')\n",
    "\n",
    "PARAMS = model_info\n",
    "\n",
    "experiment = neptune.create_experiment(name=model, params=PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = BASE_URL+'inference'\n",
    "\n",
    "os.environ['NO_PROXY'] = '127.0.0.1'\n",
    "\n",
    "val_loader = get_imagenet_val_loader('data/imagenet', batch_size=1)\n",
    "\n",
    "batch_time = exu.AverageMeter('Time', ':6.3f')\n",
    "inference_time = exu.AverageMeter('inference_time', ':6.3f')\n",
    "latency_out = exu.AverageMeter('Latency_out', ':6.3f')\n",
    "latency_back = exu.AverageMeter('Latency_back', ':6.3f')\n",
    "power_usage = exu.AverageMeter('Power_in_watts', ':.1f')\n",
    "gpu_utilization = exu.AverageMeter('GPU_utilization', ':.1f')\n",
    "top1 = exu.AverageMeter('Acc@1', ':6.2f')\n",
    "top5 = exu.AverageMeter('Acc@5', ':6.2f')\n",
    "\n",
    "progress = exu.ProgressMeter(\n",
    "    len(val_loader),\n",
    "    [batch_time, top1, top5, inference_time, latency_out, latency_back, power_usage, ],\n",
    "    prefix='Test: ')\n",
    "\n",
    "print_freq = 5000\n",
    "request_type = 'pil'\n",
    "\n",
    "class_correct = list({'top1': 0., 'top5': 0., 'total': 0.} for _ in range(len(IMAGENET_LABELS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Request Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [    0/50000]\tTime  0.128 ( 0.128)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\tinference_time  0.019 ( 0.019)\tLatency_out  0.030 ( 0.030)\tLatency_back  0.009 ( 0.009)\tPower_in_watts 57.9 (57.9)\n",
      "Test: [ 1000/50000]\tTime  0.090 ( 0.101)\tAcc@1 100.00 ( 85.21)\tAcc@5 100.00 ( 96.70)\tinference_time  0.016 ( 0.019)\tLatency_out  0.035 ( 0.037)\tLatency_back  0.013 ( 0.014)\tPower_in_watts 62.0 (60.3)\n",
      "Test: [ 2000/50000]\tTime  0.088 ( 0.099)\tAcc@1 100.00 ( 81.46)\tAcc@5 100.00 ( 96.45)\tinference_time  0.018 ( 0.019)\tLatency_out  0.042 ( 0.037)\tLatency_back  0.004 ( 0.012)\tPower_in_watts 61.1 (60.6)\n",
      "Test: [ 3000/50000]\tTime  0.114 ( 0.099)\tAcc@1   0.00 ( 78.84)\tAcc@5 100.00 ( 94.87)\tinference_time  0.019 ( 0.019)\tLatency_out  0.042 ( 0.038)\tLatency_back  0.014 ( 0.012)\tPower_in_watts 61.9 (60.9)\n",
      "Test: [ 4000/50000]\tTime  0.084 ( 0.099)\tAcc@1 100.00 ( 76.41)\tAcc@5 100.00 ( 94.40)\tinference_time  0.016 ( 0.019)\tLatency_out  0.037 ( 0.038)\tLatency_back  0.002 ( 0.011)\tPower_in_watts 62.8 (61.2)\n",
      "Test: [ 5000/50000]\tTime  0.099 ( 0.098)\tAcc@1 100.00 ( 79.18)\tAcc@5 100.00 ( 94.98)\tinference_time  0.019 ( 0.019)\tLatency_out  0.044 ( 0.039)\tLatency_back -0.000 ( 0.010)\tPower_in_watts 63.0 (61.4)\n",
      "Test: [ 6000/50000]\tTime  0.091 ( 0.099)\tAcc@1   0.00 ( 79.52)\tAcc@5 100.00 ( 94.73)\tinference_time  0.017 ( 0.019)\tLatency_out  0.037 ( 0.040)\tLatency_back  0.006 ( 0.010)\tPower_in_watts 62.6 (61.6)\n",
      "Test: [ 7000/50000]\tTime  0.090 ( 0.099)\tAcc@1 100.00 ( 80.33)\tAcc@5 100.00 ( 94.84)\tinference_time  0.019 ( 0.019)\tLatency_out  0.042 ( 0.041)\tLatency_back  0.001 ( 0.010)\tPower_in_watts 64.2 (61.7)\n",
      "Test: [ 8000/50000]\tTime  0.118 ( 0.099)\tAcc@1 100.00 ( 81.14)\tAcc@5 100.00 ( 95.11)\tinference_time  0.020 ( 0.019)\tLatency_out  0.050 ( 0.042)\tLatency_back  0.022 ( 0.009)\tPower_in_watts 60.4 (61.8)\n",
      "Test: [ 9000/50000]\tTime  0.113 ( 0.100)\tAcc@1   0.00 ( 80.78)\tAcc@5 100.00 ( 95.17)\tinference_time  0.021 ( 0.019)\tLatency_out  0.039 ( 0.042)\tLatency_back  0.010 ( 0.010)\tPower_in_watts 62.0 (61.8)\n",
      "Test: [10000/50000]\tTime  0.096 ( 0.100)\tAcc@1 100.00 ( 80.50)\tAcc@5 100.00 ( 95.15)\tinference_time  0.021 ( 0.019)\tLatency_out  0.037 ( 0.041)\tLatency_back  0.010 ( 0.010)\tPower_in_watts 60.1 (61.8)\n",
      "Test: [11000/50000]\tTime  0.100 ( 0.099)\tAcc@1 100.00 ( 80.62)\tAcc@5 100.00 ( 95.31)\tinference_time  0.022 ( 0.019)\tLatency_out  0.036 ( 0.041)\tLatency_back  0.005 ( 0.010)\tPower_in_watts 59.7 (61.8)\n",
      "Test: [12000/50000]\tTime  0.097 ( 0.100)\tAcc@1   0.00 ( 80.40)\tAcc@5 100.00 ( 95.27)\tinference_time  0.018 ( 0.019)\tLatency_out  0.037 ( 0.041)\tLatency_back  0.014 ( 0.011)\tPower_in_watts 62.2 (61.8)\n",
      "Test: [13000/50000]\tTime  0.110 ( 0.100)\tAcc@1 100.00 ( 80.53)\tAcc@5 100.00 ( 95.46)\tinference_time  0.018 ( 0.019)\tLatency_out  0.034 ( 0.041)\tLatency_back  0.026 ( 0.011)\tPower_in_watts 63.0 (61.8)\n",
      "Test: [14000/50000]\tTime  0.101 ( 0.100)\tAcc@1 100.00 ( 80.29)\tAcc@5 100.00 ( 95.49)\tinference_time  0.019 ( 0.019)\tLatency_out  0.038 ( 0.041)\tLatency_back  0.005 ( 0.011)\tPower_in_watts 63.1 (61.9)\n",
      "Test: [15000/50000]\tTime  0.100 ( 0.100)\tAcc@1 100.00 ( 80.31)\tAcc@5 100.00 ( 95.49)\tinference_time  0.017 ( 0.019)\tLatency_out  0.038 ( 0.041)\tLatency_back  0.008 ( 0.011)\tPower_in_watts 61.8 (61.9)\n",
      "Test: [16000/50000]\tTime  0.080 ( 0.100)\tAcc@1 100.00 ( 80.20)\tAcc@5 100.00 ( 95.45)\tinference_time  0.016 ( 0.019)\tLatency_out  0.039 ( 0.041)\tLatency_back  0.002 ( 0.011)\tPower_in_watts 62.9 (61.9)\n",
      "Test: [17000/50000]\tTime  0.101 ( 0.100)\tAcc@1 100.00 ( 80.72)\tAcc@5 100.00 ( 95.50)\tinference_time  0.017 ( 0.019)\tLatency_out  0.040 ( 0.041)\tLatency_back  0.000 ( 0.011)\tPower_in_watts 62.9 (61.9)\n",
      "Test: [18000/50000]\tTime  0.106 ( 0.100)\tAcc@1 100.00 ( 80.61)\tAcc@5 100.00 ( 95.56)\tinference_time  0.019 ( 0.019)\tLatency_out  0.042 ( 0.042)\tLatency_back  0.002 ( 0.010)\tPower_in_watts 62.8 (61.9)\n",
      "Test: [19000/50000]\tTime  0.128 ( 0.100)\tAcc@1 100.00 ( 80.73)\tAcc@5 100.00 ( 95.50)\tinference_time  0.020 ( 0.019)\tLatency_out  0.043 ( 0.042)\tLatency_back  0.003 ( 0.010)\tPower_in_watts 61.4 (61.9)\n",
      "Test: [20000/50000]\tTime  0.099 ( 0.100)\tAcc@1 100.00 ( 80.55)\tAcc@5 100.00 ( 95.48)\tinference_time  0.019 ( 0.019)\tLatency_out  0.034 ( 0.042)\tLatency_back  0.010 ( 0.010)\tPower_in_watts 62.6 (61.9)\n",
      "Test: [21000/50000]\tTime  0.097 ( 0.100)\tAcc@1 100.00 ( 80.12)\tAcc@5 100.00 ( 95.22)\tinference_time  0.022 ( 0.019)\tLatency_out  0.032 ( 0.042)\tLatency_back  0.016 ( 0.011)\tPower_in_watts 62.1 (61.9)\n",
      "Test: [22000/50000]\tTime  0.089 ( 0.100)\tAcc@1 100.00 ( 79.82)\tAcc@5 100.00 ( 95.06)\tinference_time  0.016 ( 0.019)\tLatency_out  0.032 ( 0.041)\tLatency_back  0.012 ( 0.011)\tPower_in_watts 62.8 (61.9)\n",
      "Test: [23000/50000]\tTime  0.085 ( 0.100)\tAcc@1   0.00 ( 79.52)\tAcc@5 100.00 ( 94.90)\tinference_time  0.017 ( 0.019)\tLatency_out  0.030 ( 0.041)\tLatency_back  0.010 ( 0.011)\tPower_in_watts 62.2 (61.9)\n",
      "Test: [24000/50000]\tTime  0.103 ( 0.100)\tAcc@1   0.00 ( 79.07)\tAcc@5   0.00 ( 94.65)\tinference_time  0.017 ( 0.019)\tLatency_out  0.031 ( 0.041)\tLatency_back  0.016 ( 0.012)\tPower_in_watts 62.6 (61.9)\n",
      "Test: [25000/50000]\tTime  0.102 ( 0.100)\tAcc@1 100.00 ( 78.41)\tAcc@5 100.00 ( 94.40)\tinference_time  0.019 ( 0.019)\tLatency_out  0.035 ( 0.040)\tLatency_back  0.013 ( 0.012)\tPower_in_watts 62.9 (61.9)\n",
      "Test: [26000/50000]\tTime  0.095 ( 0.100)\tAcc@1 100.00 ( 78.01)\tAcc@5 100.00 ( 94.23)\tinference_time  0.020 ( 0.019)\tLatency_out  0.033 ( 0.040)\tLatency_back  0.010 ( 0.012)\tPower_in_watts 63.7 (61.9)\n",
      "Test: [27000/50000]\tTime  0.092 ( 0.101)\tAcc@1 100.00 ( 77.66)\tAcc@5 100.00 ( 94.08)\tinference_time  0.020 ( 0.019)\tLatency_out  0.031 ( 0.040)\tLatency_back  0.021 ( 0.012)\tPower_in_watts 59.7 (61.9)\n",
      "Test: [28000/50000]\tTime  0.104 ( 0.101)\tAcc@1 100.00 ( 77.53)\tAcc@5 100.00 ( 93.94)\tinference_time  0.019 ( 0.019)\tLatency_out  0.034 ( 0.040)\tLatency_back  0.009 ( 0.012)\tPower_in_watts 62.5 (62.0)\n",
      "Test: [29000/50000]\tTime  0.157 ( 0.101)\tAcc@1   0.00 ( 77.58)\tAcc@5 100.00 ( 93.92)\tinference_time  0.019 ( 0.019)\tLatency_out  0.033 ( 0.040)\tLatency_back  0.057 ( 0.013)\tPower_in_watts 61.7 (62.0)\n",
      "Test: [30000/50000]\tTime  0.091 ( 0.101)\tAcc@1   0.00 ( 77.39)\tAcc@5 100.00 ( 93.77)\tinference_time  0.019 ( 0.019)\tLatency_out  0.024 ( 0.040)\tLatency_back  0.021 ( 0.013)\tPower_in_watts 62.2 (62.0)\n",
      "Test: [31000/50000]\tTime  0.101 ( 0.101)\tAcc@1   0.00 ( 77.37)\tAcc@5 100.00 ( 93.65)\tinference_time  0.019 ( 0.019)\tLatency_out  0.022 ( 0.039)\tLatency_back  0.029 ( 0.013)\tPower_in_watts 64.7 (62.0)\n",
      "Test: [32000/50000]\tTime  0.118 ( 0.100)\tAcc@1 100.00 ( 76.97)\tAcc@5 100.00 ( 93.49)\tinference_time  0.022 ( 0.019)\tLatency_out  0.022 ( 0.039)\tLatency_back  0.031 ( 0.014)\tPower_in_watts 60.9 (62.0)\n",
      "Test: [33000/50000]\tTime  0.102 ( 0.100)\tAcc@1 100.00 ( 76.74)\tAcc@5 100.00 ( 93.38)\tinference_time  0.018 ( 0.019)\tLatency_out  0.032 ( 0.038)\tLatency_back  0.017 ( 0.014)\tPower_in_watts 62.0 (62.0)\n",
      "Test: [34000/50000]\tTime  0.085 ( 0.100)\tAcc@1 100.00 ( 76.59)\tAcc@5 100.00 ( 93.31)\tinference_time  0.019 ( 0.019)\tLatency_out  0.025 ( 0.038)\tLatency_back  0.022 ( 0.014)\tPower_in_watts 60.5 (62.0)\n",
      "Test: [35000/50000]\tTime  0.093 ( 0.100)\tAcc@1 100.00 ( 76.54)\tAcc@5 100.00 ( 93.29)\tinference_time  0.019 ( 0.019)\tLatency_out  0.024 ( 0.038)\tLatency_back  0.026 ( 0.015)\tPower_in_watts 62.6 (62.0)\n",
      "Test: [36000/50000]\tTime  0.095 ( 0.100)\tAcc@1 100.00 ( 76.44)\tAcc@5 100.00 ( 93.23)\tinference_time  0.021 ( 0.019)\tLatency_out  0.027 ( 0.038)\tLatency_back  0.022 ( 0.015)\tPower_in_watts 59.4 (62.0)\n",
      "Test: [37000/50000]\tTime  0.101 ( 0.100)\tAcc@1   0.00 ( 76.31)\tAcc@5   0.00 ( 93.13)\tinference_time  0.016 ( 0.019)\tLatency_out  0.029 ( 0.037)\tLatency_back  0.025 ( 0.015)\tPower_in_watts 62.9 (62.0)\n",
      "Test: [38000/50000]\tTime  0.088 ( 0.100)\tAcc@1   0.00 ( 76.08)\tAcc@5   0.00 ( 93.01)\tinference_time  0.017 ( 0.019)\tLatency_out  0.026 ( 0.037)\tLatency_back  0.019 ( 0.015)\tPower_in_watts 61.6 (62.0)\n",
      "Test: [39000/50000]\tTime  0.089 ( 0.100)\tAcc@1   0.00 ( 75.97)\tAcc@5 100.00 ( 92.90)\tinference_time  0.019 ( 0.019)\tLatency_out  0.032 ( 0.037)\tLatency_back  0.012 ( 0.016)\tPower_in_watts 61.9 (62.0)\n",
      "Test: [40000/50000]\tTime  0.106 ( 0.100)\tAcc@1 100.00 ( 75.79)\tAcc@5 100.00 ( 92.76)\tinference_time  0.020 ( 0.019)\tLatency_out  0.032 ( 0.037)\tLatency_back  0.014 ( 0.016)\tPower_in_watts 62.1 (62.0)\n",
      "Test: [41000/50000]\tTime  0.237 ( 0.100)\tAcc@1 100.00 ( 75.60)\tAcc@5 100.00 ( 92.69)\tinference_time  0.017 ( 0.019)\tLatency_out  0.142 ( 0.037)\tLatency_back  0.039 ( 0.016)\tPower_in_watts 63.0 (62.0)\n",
      "Test: [42000/50000]\tTime  0.096 ( 0.100)\tAcc@1 100.00 ( 75.43)\tAcc@5 100.00 ( 92.58)\tinference_time  0.023 ( 0.019)\tLatency_out  0.023 ( 0.037)\tLatency_back  0.018 ( 0.016)\tPower_in_watts 58.2 (62.0)\n",
      "Test: [43000/50000]\tTime  0.103 ( 0.100)\tAcc@1   0.00 ( 75.37)\tAcc@5 100.00 ( 92.51)\tinference_time  0.018 ( 0.019)\tLatency_out  0.022 ( 0.036)\tLatency_back  0.033 ( 0.016)\tPower_in_watts 64.6 (62.0)\n",
      "Test: [44000/50000]\tTime  0.094 ( 0.100)\tAcc@1 100.00 ( 75.26)\tAcc@5 100.00 ( 92.48)\tinference_time  0.018 ( 0.019)\tLatency_out  0.025 ( 0.036)\tLatency_back  0.017 ( 0.016)\tPower_in_watts 62.8 (62.0)\n",
      "Test: [45000/50000]\tTime  0.084 ( 0.100)\tAcc@1 100.00 ( 75.11)\tAcc@5 100.00 ( 92.39)\tinference_time  0.019 ( 0.019)\tLatency_out  0.026 ( 0.036)\tLatency_back  0.014 ( 0.017)\tPower_in_watts 62.3 (62.0)\n",
      "Test: [46000/50000]\tTime  0.090 ( 0.100)\tAcc@1   0.00 ( 74.97)\tAcc@5   0.00 ( 92.34)\tinference_time  0.017 ( 0.019)\tLatency_out  0.027 ( 0.036)\tLatency_back  0.015 ( 0.017)\tPower_in_watts 62.2 (62.0)\n",
      "Test: [47000/50000]\tTime  0.095 ( 0.099)\tAcc@1   0.00 ( 74.94)\tAcc@5 100.00 ( 92.36)\tinference_time  0.020 ( 0.019)\tLatency_out  0.032 ( 0.036)\tLatency_back  0.015 ( 0.017)\tPower_in_watts 62.6 (62.0)\n",
      "Test: [48000/50000]\tTime  0.097 ( 0.099)\tAcc@1 100.00 ( 75.02)\tAcc@5 100.00 ( 92.41)\tinference_time  0.021 ( 0.019)\tLatency_out  0.034 ( 0.035)\tLatency_back  0.016 ( 0.017)\tPower_in_watts 59.9 (62.0)\n",
      "Test: [49000/50000]\tTime  0.081 ( 0.099)\tAcc@1 100.00 ( 74.78)\tAcc@5 100.00 ( 92.31)\tinference_time  0.016 ( 0.019)\tLatency_out  0.031 ( 0.035)\tLatency_back  0.010 ( 0.017)\tPower_in_watts 62.4 (62.0)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'neptune' has no attribute 'end'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-fce6d7de6a9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mclass_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'top1_accuracy'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'top1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'top5_accuracy'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'top5'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mIMAGENET_LABELS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_correct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mneptune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'class_accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m \u001b[0mneptune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'neptune' has no attribute 'end'"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    end = time.time()\n",
    "    for i, (images, labels) in enumerate(val_loader):\n",
    "        \n",
    "        # check if flag is json vs pil\n",
    "        if request_type == 'json':\n",
    "            im = images[0]\n",
    "            payload = json.dumps({\"image\": im.tolist()}).encode('utf-8')\n",
    "            time_request_sent = time.time()\n",
    "            r = requests.post(url, data=payload)\n",
    "        elif request_type == 'pil':\n",
    "            im = transforms.ToPILImage()(images[0])\n",
    "\n",
    "            # convert pil image to bytes\n",
    "            with BytesIO() as output:\n",
    "                im.save(output, 'JPEG')\n",
    "                data = output.getvalue()\n",
    "\n",
    "\n",
    "            payload = {\n",
    "                'files': (\n",
    "                    '1.jpeg',\n",
    "                    data,\n",
    "                    'image/jpeg'\n",
    "                )\n",
    "            }\n",
    "            time_request_sent = time.time()\n",
    "            r = requests.post(url, files=payload)\n",
    "        time_response_recieved = time.time()\n",
    "        r_json = r.json()\n",
    "        \n",
    "        outputs = torch.tensor(\n",
    "            r_json['result']['prediction_raw']).unsqueeze(0)\n",
    "        \n",
    "        outputs, labels = outputs.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        acc1, acc5 = exu.accuracy(outputs, labels, topk=(1, 5))\n",
    "        \n",
    "#         print(torch.nn.functional.softmax(outputs).topk(5))\n",
    "        \n",
    "        class_correct[labels[0].item()]['total'] += 1\n",
    "        \n",
    "        if acc1[0] == 100.:\n",
    "            class_correct[labels[0].item()]['top1'] += 1   \n",
    "        \n",
    "        if acc5[0] == 100.:\n",
    "            class_correct[labels[0].item()]['top5'] += 1   \n",
    "            \n",
    "        top1.update(acc1[0], images.size(0))\n",
    "        top5.update(acc5[0], images.size(0))\n",
    "        \n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        inference_time.update(r_json['result']['inference_time_ms'])\n",
    "        \n",
    "        power_usage.update(r_json['meta']['cuda_info']['gpu'][0]['power_readings']['power_draw'], images.size(0))\n",
    "        gpu_utilization.update(r_json['meta']['cuda_info']['gpu'][0]['utilization']['gpu_util'], images.size(0))\n",
    "        \n",
    "        latency_out.update(r_json['meta']['time_request_recieved']-time_request_sent)\n",
    "        latency_back.update(time_response_recieved-r_json['meta']['time_response_sent'])\n",
    "        \n",
    "        if i % print_freq == 0:\n",
    "            progress.display(i)\n",
    "        \n",
    "        neptune.log_metric('batch_time', batch_time.val)\n",
    "        neptune.log_metric('inference_time', inference_time.val)\n",
    "        neptune.log_metric('latency_out', latency_out.val)\n",
    "        neptune.log_metric('latency_back', latency_back.val)\n",
    "        neptune.log_metric('top1_accuracy_avg', top1.avg)\n",
    "        neptune.log_metric('top1_accuracy_raw', top1.val)\n",
    "        neptune.log_metric('top5_accuracy_avg', top5.avg)\n",
    "        neptune.log_metric('top5_accuracy_raw', top5.val)\n",
    "        neptune.log_metric('gpu_power_w', power_usage.val)\n",
    "        neptune.log_metric('gpu_util', gpu_utilization.val)\n",
    "    \n",
    "    results = {\n",
    "       r_json['meta']['model']['name']: {\n",
    "           'batch_time': {\n",
    "               'avg': batch_time.avg,\n",
    "               'max': batch_time.max,\n",
    "               'min': batch_time.min\n",
    "           },\n",
    "           'inference_time': {\n",
    "               'avg': inference_time.avg,\n",
    "               'max': inference_time.max,\n",
    "               'min': inference_time.min\n",
    "           },\n",
    "           'latency_out': {\n",
    "               'avg': latency_out.avg,\n",
    "               'max': latency_out.max,\n",
    "               'min': latency_out.min\n",
    "           },\n",
    "           'latency_back': {\n",
    "               'avg': latency_back.avg,\n",
    "               'max': latency_back.max,\n",
    "               'min': latency_back.min\n",
    "           },\n",
    "           'top1_accuracy': {\n",
    "               'avg': top1.avg.item(),\n",
    "           },\n",
    "           'top5_accuracy': {\n",
    "               'avg': top5.avg.item(),\n",
    "           },\n",
    "           'gpu_power_usage': {\n",
    "               'avg': power_usage.avg,\n",
    "               'max': power_usage.max,\n",
    "               'min': power_usage.min\n",
    "           },\n",
    "           'gpu_util': {\n",
    "               'avg': gpu_utilization.avg,\n",
    "               'max': gpu_utilization.max,\n",
    "               'min': gpu_utilization.min\n",
    "           },\n",
    "       } \n",
    "    }\n",
    "    neptune.log_text('results',json.dumps(results))\n",
    "    class_accuracy = '\\n'.join([json.dumps(dict(x, **{'top1_accuracy': 100*x['top1']/x['total'], 'top5_accuracy': 100*x['top5']/x['total']},**{\"class\":IMAGENET_LABELS[i]})) for i,x in enumerate(class_correct)])\n",
    "    neptune.log_text('class_accuracy', class_accuracy)\n",
    "neptune.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "neptune": {
   "notebookId": "69878332-eea1-4a11-afe3-8aef4a6be839"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
