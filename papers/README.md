# Papers I'm Reading

## Neural Network Compression

### Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding

[Link](https://arxiv.org/abs/1510.00149)

### Channel Pruning for Accelerating Very Deep Neural Networks

[Link](https://arxiv.org/abs/1707.06168)

### Growing and pruning neural tree networks

[Link](https://ieeexplore.ieee.org/abstract/document/210172)

### Learning both Weihts and Connections for Efficient Neural Networks

[Link](http://papers.nips.cc/paper/5784-learning-both-weights-and-connections-for-efficient-neural-network)

### Optimal Brain Damage

[Link](http://yann.lecun.com/exdb/publis/pdf/lecun-90b.pdf)

### Universal Deep Neural Network Compression

### Structured Pruning of Deep Convolutional Neural Networks

### Constraint-aware deep neural network compression

### A survey of model compression and acceleration for deep neural networks

### Pruning neural networks: is it time to nip it in the bud

### The lottery ticket hypothesis: Finding sparse, trainable neural networks

### Quantized neural networks: Training neural networks with low precision weights and activations

### To prune, or not to prune: exploring the efficacy of pruning for model compression
