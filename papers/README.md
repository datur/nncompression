# Papers I'm Reading

## Neural Network Compression

| Title                                                                                                    | Author             | Link to Paper                                                                                               | Notes                                                                                                  |
| -------------------------------------------------------------------------------------------------------- | ------------------ | ----------------------------------------------------------------------------------------------------------- |
| Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding | Han et al.         | [Link](https://arxiv.org/abs/1510.00149)                                                                    | [Link](notes/Compressing_Deep_Neural_Networks_with_Pruning-Trained_Quantization_and_Huffman_Coding.md) |
| Channel Pruning for Accelerating Very Deep Neural Networks                                               | He et al.          | [Link](https://arxiv.org/abs/1707.06168)                                                                    | [Link](notes/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks.md)                            |
| Growing and pruning neural tree networks                                                                 | Sakar and Mammone  | [Link](https://ieeexplore.ieee.org/abstract/document/210172)                                                |
| Learning both Weights and Connections for Efficient Neural Networks                                      | Han et al.         | [Link](http://papers.nips.cc/paper/5784-learning-both-weights-and-connections-for-efficient-neural-network) |
| Optimal Brain Damage                                                                                     | Le Chun et al.     | [Link](http://yann.lecun.com/exdb/publis/pdf/lecun-90b.pdf)                                                 |
| Universal Deep Neural Network Compression                                                                | Choi et al.        | [Link](https://arxiv.org/abs/1802.02271)                                                                    |
| Structured Pruning of Deep Convolutional Neural Networks                                                 | Anwar et al.       | [Link](https://arxiv.org/abs/1512.08571)                                                                    |
| Constraint-aware deep neural network compression                                                         | Chen et al.        | [Link](http://www.sfu.ca/~ftung/papers/constraintaware_eccv18.pdf)                                          |
| A survey of model compression and acceleration for deep neural networks                                  | Cheng et al        | [Link](https://arxiv.org/abs/1710.09282)                                                                    |
| Pruning neural networks: is it time to nip it in the bud                                                 | Crowley et al      | [Link](https://openreview.net/pdf?id=r1lbgwFj5m)                                                            |
| The lottery ticket hypothesis: Finding sparse, trainable neural networks                                 | Frankle and Carbin | [Link](https://arxiv.org/abs/1803.03635)                                                                    |
| Quantized neural networks: Training neural networks with low precision weights and activations           | Hubara et al.      | [Link](https://arxiv.org/abs/1609.07061)                                                                    |
| To prune, or not to prune: exploring the efficacy of pruning for model compression                       | Zhu and Gupta      | [Link](https://arxiv.org/abs/1710.01878)                                                                    |

